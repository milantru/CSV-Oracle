\chapter{Experimenty}

Táto kapitola najprv predstavuje štruktúru experimentov, po ktorej nasleduje analýza meraní a prezentácia výsledkov.

\section{Štruktúra experimentov}

Budeme pracovať s dvoma datasetmi:
\begin{enumerate}
\item COVID-19 dataset
\item Population structure 2011
\end{enumerate}

Pre prvý dataset máme k dispozícii schému. Druhý patrí medzi high-value datasety a~pozostáva z~viacerých CSV súborov, schému nemáme.

V experimentoch budeme testovať ako sa zmení kvalita odpovedí asistenta keď je prítomná schéma. Okrem toho, budeme takisto sledovať ako sa zmení kvalita odpovedí asistenta pri poskytnutí rôznych variant nástrojov.

Vytvoríme si varianty experimentov:
\begin{enumerate}
\item COVID-19 dataset bez schémy, využijeme všetky indexy (CSV súbory, data profiling reporty, schéma), 
\begin{enumerate}
\item 
\item 
\end{enumerate}
\item COVID-19 dataset so schémou
\item Population structure 2011
\end{enumerate}


Môžeme skúsiť využiť:
všetky 3 indexy (csv files, data profiling reports, schema)
2 indexy (csv files, data profiling reports)

Ďalšia možnosť:
využiť individuálne query tooly [exp 2]
využiť sub question query engine tool tvorený individuálnymi toolmi [exp 3]
využiť individuálne tooly + sub question query engine tool, ktorý je nimi tvorený [exp1]

Kontext pre ďalšiu možnosť:
Query tool dostane otázku a vráti odpoveď, využíva 1 zdroj informácií (napr. raw csv files).
Sub question query engine tool je tvorený viacerými query toolmi. Takisto dostane otázku, vytvorí podotázky, ktoré dá query toolom (môže všetkým, ktoré má k dispozícii alebo len niektorým), tie mu vrátia odpovede a na základe nich vytvorí odpoveď na pôvodnú otázku.
Momentálne používam len poslednú možnosť (kombináciu query toolov + sub question query engine). Využitie nejakého query toolu trvá. Mám za to, že ak by sa využil iba sub question query engine tool, generovanie odpovede by trvalo dlho, ale teoreticky by to malo zlepšiť kvalitu (pravdou je, že som objavil situáciu kedy tomu tak byť nemusí, ale skôr šlo o veľmi špeciálny edge case, tým chcem povedať, že to nie je také jednoznačné). Aby som zrýchlil generovanie odpovede dávam LLMku k dispozícii aj individuálne query tooly. Ale ako som spomínal, sub queston query engine tool nemusí využiť všetky tooly, ktoré má k dispozícii. Asi je tam síce stále overhead generovania podotázky, ale možno by to z časového hľadiska nebolo tak zlé. Každopádne, zaujímalo by ma, či by sa zlepšila kvalita odpovede keby zmením tooly tak ako som to opísal vyššie.

Ak by sme zahrnuli aj poslednú možnosť, bolo by 18 experimentov.

Ako by experiment prebiehal?
Nahráme dataset (príp. aj schému). Akonáhle program dokončí analýzu a vytvorí dataset knowledge, keďže má dataset knowledge pevnú štruktúru, môžeme skontrolovať ako dobre je vytvorená, či nechýbajú nejaké “políčka” (To by sa malo dať vyjadriť napr. v percentách.)
Ďalej pre každý dataset si pripravíme sériu otázok/promptov a očakávaných odpovedí/reakcií (celkovo budeme mať teda 2 série). V každom experimente sa budeme pýtať predpripravené otázky a porovnáme odpovede (resp. reakcie ak budeme chcieť testovať aj update dataset knowledge, čo predpokladám, že budeme).

Séria otázok 1:
User view: “I want to use this dataset to create a simple dashboard with charts that show how COVID-19 developed over time, e.g. cases, deaths, and recoveries.”

Is it possible to show total infections, deaths, and recoveries in my app using this dataset? Which columns contain that information?
Yes, it’s possible. To show the total number of infections, use kumulovany_pocet_nakazenych. For total deaths, use kumulovany_pocet_umrti, and for total recoveries, use kumulovany_pocet_vylecenych. These columns contain cumulative values, so they reflect the total counts up to each date.

Is it possible to calculate active COVID-19 cases from this dataset? If so, how?
Active cases can be calculated as: active_cases = kumulovany_pocet_nakazenych - kumulovany_pocet_vylecenych - kumulovany_pocet_umrti. This gives the total number of people still infected at a given point.

Is it possible to detect spikes or anomalies in the daily case numbers using this dataset? If so, how?
Spikes can be detected by calculating the daily differences using prirustkovy_pocet_nakazenych and flagging days where the daily change exceeds a predefined threshold (e.g. 2x the rolling average of daily cases).

What are some interesting metrics I could derive from this dataset and display in charts? // TODO Asi 4. prepíš na jednotlivé otázky ako v alternatíve k 4. máš
Test Positivity Rate: test positivity = (prirustkovy_pocet_nakazenych / prirustkovy_pocet_provedenych_testu) * 100
Recovery Rate: recovery rate = (kumulovany_pocet_vylecenych / kumulovany_pocet_nakazenych) * 100 
Fatality Rate: fatality rate = (kumulovany_pocet_umrti / kumulovany_pocet_nakazenych) * 100 
Testing Efficiency: testing efficiency = (prirustkovy_pocet_nakazenych / prirustkovy_pocet_provedenych_testu)th

(Alternatíva k 4.) Can I estimate the test positivity rate from this data? If so, how can I calculate it, and which columns should I use?
Yes, the test positivity rate can be calculated as: test positivity rate = (prirustkovy_pocet_nakazenych / prirustkovy_pocet_provedenych_testu) * 100 This gives the percentage of tests that resulted in positive cases.
OTÁZKA: Čo ak využije kumulatívny namiesto prírastkový? Asi tiež brať ako validné, alebo nie?

What column shows how many COVID-19 tests were done each day?
You should use the column prirustkovy_pocet_provedenych_testu to see how many PCR tests were done each day. If you also want to include antigen tests, use prirustkovy_pocet_provedenych_ag_testu. To get the total number of tests per day, you can sum both columns.

Can I use this dataset to analyze which weekdays have the most and fewest COVID-19 tests performed?
Yes. is is possible.

Can you please add this last information to the dataset knowledge to the mentioned column?
Yes, the dataset knowledge has been updated. [It really is updated correctly with the mentioned information.]

Séria otázok 2:
User view: I would like to use this dataset to create an application that visualizes the population structure of different urban areas, allowing users to explore demographic details such as average age and youth and elderly population ratios.

Can I calculate the average age for each urban area using this dataset? If so, which columns could I use for that?
Yes, you can calculate the average age for each urban area. The relevant column for this calculation is durchschnittsalter, which provides the average age for each urban area.

Is it possible to create a heatmap that shows areas with the highest and lowest youth and elderly population ratios? If so, which columns should I use?
Yes, you can. You would use the jugendquotient column for youth population ratio and the altenquotient column for elderly population ratio. These will help identify areas with the highest and lowest ratios.

Is it possible to view the areas and their percentages of households with children? If so, which columns should I use?
Yes, you can. The relevant column is anteil_mit_kindern, which shows the percentage of households with children in each urban area.

Is it possible to identify which urban areas have a higher concentration of younger populations (under 30)? Which columns should I use?
Yes, you can. The best columns to use are anzahl_juenger_3, anzahl_3_6, anzahl_6_15, and anzahl_15_25 to identify the number of younger people in specific age ranges. You can then calculate the concentration of younger populations in each area.

Is it possible to visualize areas with the percentages of men and women? If so, which columns should I use?
Yes, it is possible. You can use the columns anteil_maennlich (percentage of male population) and anteil_weiblich (percentage of female population) from the "Population by gender 2011" table. You can also include the corresponding urban area from stadtbereich_bezeichnung.

Can I display the proportion of foreign vs. German residents by area? Which columns should I use?
Yes, you can use anteil_deutsch (percentage of German residents) and anteil_auslaendisch (percentage of foreign residents) from the "Spatial Population Movement 2011" table.

Can you add this last information to the dataset knowledge to the referenced columns?
Yes, the dataset knowledge has been updated. [It really is updated correctly with the mentioned information.]

// TODO/OTÁZKA Séria 2 je k datasetu, ktorý má viaceré csv súbory (nevyužívame všetky, pravdepodobne len 4). Pri posledných odpovediach je spomenutý nie len názov stĺpca, ale aj názov tabuľky. Ako budeme vyhodnocovať? Ak užívateľ nešpecifikuje, ze chce aj názov tabuľky, nemusí tam byť?
// TODO 0,1,2

Dimenzie experimentov by sa dali rozšíriť s:
viacero datasetmi (líšiacimi sa obsahom, veľkosťou súboru, počtom súborov)
viacerými kombináciami indexov (napr. iba csv files)
rôznymi promptovacími technikami (zmena system promptov, promptov pri analýze)
viacerými rôznymi LLMs, čo do typu alebo veľkosti (gpt-3.5, llama 3.2, llama 3.3…)
viacerými otázkami
